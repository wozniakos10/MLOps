{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b41530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.2)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers onnxruntime onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19e536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 20 16:54:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9961cd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2545a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import Literal\n",
    "import onnxruntime as ort\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0480c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawidwozniak/studies/sem9/MLOps/lab07/laboratory/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160be28e",
   "metadata": {},
   "source": [
    "# 1. Evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98a0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b4e55e24e643bd967b103be4a860e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ab4f4c68f54eef8ad2d2fa7e1eb042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6521cf4dfa64fde9b26e0feb44323c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57a2a1dce842758377199ee50b3ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52030637c43945289f3171356aea6a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f3738ce16346f3aa2cf433fbb3874c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4afc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"Transformers are amazing for natural language processing tasks.\",\n",
    "    \"ArithmeticError is a built-in exception in Python.\",\n",
    "    \"Short texts work well too!\",\n",
    "]\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6617d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '[UNK]',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecaec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Token: <s>\n",
      "ID: 2, Token: </s>\n",
      "ID: 104, Token: [UNK]\n",
      "ID: 1, Token: <pad>\n",
      "ID: 30526, Token: <mask>\n"
     ]
    }
   ],
   "source": [
    "# Checking special ids and tokens\n",
    "special_ids = tokenizer.all_special_ids\n",
    "for idx in special_ids:\n",
    "    print(f\"ID: {idx}, Token: {tokenizer.convert_ids_to_tokens(idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd40db9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 19085,  2028,  6433,  2009,  3023,  2657,  6368,  8522,  1016,\n",
       "             2,     1,     1,     1],\n",
       "        [    0, 20208,  2125, 29169,  2007,  1041,  2332,  1015,  2003,  6457,\n",
       "          2003, 18754,  1016,     2],\n",
       "        [    0,  2464,  6985,  2151,  2096,  2209,  1003,     2,     1,     1,\n",
       "             1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "# Just checking if special tokens were added\n",
    "# 0 - bos_token\n",
    "# 1 - pad_token\n",
    "# 2 - eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71995610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf51a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_basic_pytorch_time(model, inputs, device, n_iters=100):\n",
    "    model.to(device)\n",
    "    inputs.to(device)\n",
    "\n",
    "    time_lst = []\n",
    "    for _ in range(n_iters):\n",
    "        start = time.time()\n",
    "        _ = model(**inputs)\n",
    "        end = time.time()\n",
    "        time_lst.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    print(\n",
    "        f\"(default) Average inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\"\n",
    "    )\n",
    "    return avg_time, std_time\n",
    "\n",
    "\n",
    "def compare_pytorch_model_eval(\n",
    "    model, inputs, device, compile_model: bool = False, n_iters=100\n",
    "):\n",
    "    model.to(device)\n",
    "    inputs.to(device)\n",
    "    time_lst = []\n",
    "    model.eval()\n",
    "\n",
    "    if compile_model:\n",
    "        start = time.time()\n",
    "        model = torch.compile(model)\n",
    "        _ = model(**inputs)  # Warm-up after compilation\n",
    "        end = time.time()\n",
    "        print(f\"Model compilation time and warm-up: {end - start:.6f} seconds\")\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        start = time.time()\n",
    "        _ = model(**inputs)\n",
    "        end = time.time()\n",
    "        time_lst.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    text = \"(eval mode + compiled)\" if compile_model else \"(eval mode)\"\n",
    "    print(\n",
    "        f\"{text} Average inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\"\n",
    "    )\n",
    "    return avg_time, std_time\n",
    "\n",
    "\n",
    "def compare_pytorch_model_eval_no_grad(\n",
    "    model, inputs, device, compile_model: bool = False, n_iters=100\n",
    "):\n",
    "    model.to(device)\n",
    "    inputs.to(device)\n",
    "    time_lst = []\n",
    "    model.eval()\n",
    "\n",
    "    if compile_model:\n",
    "        start = time.time()\n",
    "        model = torch.compile(model)\n",
    "        _ = model(**inputs)  # Warm-up after compilation\n",
    "        end = time.time()\n",
    "        print(f\"Model compilation time and warm-up: {end - start:.6f} seconds\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            start = time.time()\n",
    "            _ = model(**inputs)\n",
    "            end = time.time()\n",
    "            time_lst.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    print_text = (\n",
    "        \"(eval mode + no_grad + compiled)\" if compile_model else \"(eval mode + no_grad)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{print_text} Average inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\"\n",
    "    )\n",
    "    return avg_time, std_time\n",
    "\n",
    "\n",
    "def compare_pytorch_model_eval_inference_mode(\n",
    "    model, inputs, device, compile_model: bool = False, n_iters=100\n",
    "):\n",
    "    model.to(device)\n",
    "    inputs.to(device)\n",
    "    time_lst = []\n",
    "    model.eval()\n",
    "\n",
    "    if compile_model:\n",
    "        start = time.time()\n",
    "        model = torch.compile(model)\n",
    "        _ = model(**inputs)  # Warm-up after compilation\n",
    "        end = time.time()\n",
    "        print(f\"Model compilation time and warm-up: {end - start:.6f} seconds\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(n_iters):\n",
    "            start = time.time()\n",
    "            _ = model(**inputs)\n",
    "            end = time.time()\n",
    "            time_lst.append(end - start)\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    print_text = (\n",
    "        \"(eval mode + inference_mode + compiled)\"\n",
    "        if compile_model\n",
    "        else \"(eval mode + inference_mode)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{print_text} Average inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return avg_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0571a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_different_inference_modes(model, inputs, device, n_iters=100):\n",
    "    print(\"Comparing different inference modes:\")\n",
    "    basic_results = compare_basic_pytorch_time(model, inputs, device, n_iters=n_iters)\n",
    "    eval_results = compare_pytorch_model_eval(\n",
    "        model, inputs, device, compile_model=False, n_iters=n_iters\n",
    "    )\n",
    "    eval_no_grad_results = compare_pytorch_model_eval_no_grad(\n",
    "        model, inputs, device, compile_model=False, n_iters=n_iters\n",
    "    )\n",
    "    eval_inference_mode_results = compare_pytorch_model_eval_inference_mode(\n",
    "        model, inputs, device, compile_model=False, n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    eval_speedup = basic_results[0] / eval_results[0]\n",
    "    eval_no_grad_speedup = basic_results[0] / eval_no_grad_results[0]\n",
    "    eval_inference_mode_speedup = basic_results[0] / eval_inference_mode_results[0]\n",
    "\n",
    "    print(f\"Speedup with eval mode: {eval_speedup:.2f}x\")\n",
    "    print(f\"Speedup with eval mode + no_grad: {eval_no_grad_speedup:.2f}x\")\n",
    "    print(\n",
    "        f\"Speedup with eval mode + inference_mode: {eval_inference_mode_speedup:.2f}x\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c951e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different inference modes:\n",
      "(default) Average inference time over 100 iterations: 0.011900 seconds, std: 0.011750 seconds\n",
      "(eval mode) Average inference time over 100 iterations: 0.010750 seconds, std: 0.001262 seconds\n",
      "(eval mode + no_grad) Average inference time over 100 iterations: 0.008669 seconds, std: 0.000985 seconds\n",
      "(eval mode + inference_mode) Average inference time over 100 iterations: 0.007724 seconds, std: 0.000973 seconds\n",
      "\n",
      "Speedup with eval mode: 1.11x\n",
      "Speedup with eval mode + no_grad: 1.37x\n",
      "Speedup with eval mode + inference_mode: 1.54x\n"
     ]
    }
   ],
   "source": [
    "compare_different_inference_modes(model, inputs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1af1",
   "metadata": {},
   "source": [
    "Each successive inference mode improved execution speed. This shows that with just two simple changes — calling model.eval() and using torch.inference_mode() — we can nearly halve inference time. We should always run models in evaluation and inference mode when serving them because it's very easy change in code and gave us a lot if we think that we can save e.g half of our cost spend on VM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02727f2",
   "metadata": {},
   "source": [
    "# 2. PyTorch model compilation <a id=\"pytorch-model-compilation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfac6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pytorch_compile_speedup(model, inputs, device, n_iters=100):\n",
    "    print(\"Comparing PyTorch compilation speedup:\")\n",
    "    basic_results = compare_basic_pytorch_time(model, inputs, device, n_iters=n_iters)\n",
    "    eval_results = compare_pytorch_model_eval(\n",
    "        model, inputs, device, compile_model=True, n_iters=n_iters\n",
    "    )\n",
    "    eval_no_grad_results = compare_pytorch_model_eval_no_grad(\n",
    "        model, inputs, device, compile_model=True, n_iters=n_iters\n",
    "    )\n",
    "    eval_inference_mode_results = compare_pytorch_model_eval_inference_mode(\n",
    "        model, inputs, device, compile_model=True, n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    eval_speedup = basic_results[0] / eval_results[0]\n",
    "    eval_no_grad_speedup = basic_results[0] / eval_no_grad_results[0]\n",
    "    eval_inference_mode_speedup = basic_results[0] / eval_inference_mode_results[0]\n",
    "\n",
    "    print(f\"Speedup with eval mode + compilation: {eval_speedup:.2f}x\")\n",
    "    print(\n",
    "        f\"Speedup with eval mode + no_grad + compilation: {eval_no_grad_speedup:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup with eval mode + inference_mode + compilation: {eval_inference_mode_speedup:.2f}x\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79352612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing PyTorch compilation speedup:\n",
      "(default) Average inference time over 100 iterations: 0.010302 seconds, std: 0.001018 seconds\n",
      "Model compilation time and warm-up: 15.941382 seconds\n",
      "(eval mode + compiled) Average inference time over 100 iterations: 0.006355 seconds, std: 0.001495 seconds\n",
      "Model compilation time and warm-up: 0.006354 seconds\n",
      "(eval mode + no_grad + compiled) Average inference time over 100 iterations: 0.024031 seconds, std: 0.180440 seconds\n",
      "Model compilation time and warm-up: 0.006768 seconds\n",
      "(eval mode + inference_mode + compiled) Average inference time over 100 iterations: 0.023997 seconds, std: 0.179867 seconds\n",
      "\n",
      "Speedup with eval mode + compilation: 1.62x\n",
      "Speedup with eval mode + no_grad + compilation: 0.43x\n",
      "Speedup with eval mode + inference_mode + compilation: 0.43x\n"
     ]
    }
   ],
   "source": [
    "# First run\n",
    "compare_pytorch_compile_speedup(model, inputs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing PyTorch compilation speedup:\n",
      "(default) Average inference time over 100 iterations: 0.010652 seconds, std: 0.001046 seconds\n",
      "Model compilation time and warm-up: 0.007268 seconds\n",
      "(eval mode + compiled) Average inference time over 100 iterations: 0.005664 seconds, std: 0.000649 seconds\n",
      "Model compilation time and warm-up: 0.006289 seconds\n",
      "(eval mode + no_grad + compiled) Average inference time over 100 iterations: 0.005666 seconds, std: 0.000471 seconds\n",
      "Model compilation time and warm-up: 0.006843 seconds\n",
      "(eval mode + inference_mode + compiled) Average inference time over 100 iterations: 0.005778 seconds, std: 0.000270 seconds\n",
      "\n",
      "Speedup with eval mode + compilation: 1.88x\n",
      "Speedup with eval mode + no_grad + compilation: 1.88x\n",
      "Speedup with eval mode + inference_mode + compilation: 1.84x\n"
     ]
    }
   ],
   "source": [
    "# Second run\n",
    "compare_pytorch_compile_speedup(model, inputs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efd8f1",
   "metadata": {},
   "source": [
    "Using `model.compile()` yields even larger speedups—roughly 2×—compared to the ~1.5× observed for eval mode + inference mode. With compilation, the differences between other optimization methods diminish, and I no longer see consistent improvements when stacking them. This may be because `model.compile()` applies such strong optimizations that contexts like `torch.no_grad()` or `torch.inference_mode()` have less impact.\n",
    "\n",
    "Compilation and warm-up take a noticeable amount of time (≈15 seconds), but this cost is incurred only once; the compiled model can then be reused (cached) for serving. On the first run, I observed a speedup only for the eval mode + compilation configuration, while other combinations were slower. I initially thought this was a mistake, but I couldn't find issues in my code. On the second run, timings were consistent. I lack a clear explanation for the initial anomaly, especially since the explicit warm-up time was excluded from the inference measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d7a1",
   "metadata": {},
   "source": [
    "# 3. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791a3e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 19085,  2028,  6433,  2009,  3023,  2657,  6368,  8522,  1016,\n",
       "             2,     1,     1,     1],\n",
       "        [    0, 20208,  2125, 29169,  2007,  1041,  2332,  1015,  2003,  6457,\n",
       "          2003, 18754,  1016,     2],\n",
       "        [    0,  2464,  6985,  2151,  2096,  2209,  1003,     2,     1,     1,\n",
       "             1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure that model runs on cpu in the exercise 3\n",
    "device = \"cpu\"\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "model.to(device)\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5721998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1794326240.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qunatized_model = torch.ao.quantization.quantize_dynamic(model, dtype=torch.qint8, qconfig_spec={nn.Linear})\n"
     ]
    }
   ],
   "source": [
    "qunatized_model = torch.ao.quantization.quantize_dynamic(\n",
    "    model, dtype=torch.qint8, qconfig_spec={nn.Linear}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21be4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f4a6f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qunatized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77973b",
   "metadata": {},
   "source": [
    "As we can see, in quantized model there are `dtype=torch.qint8` types in weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad4854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_32_path = \"fp32_model.pth\"\n",
    "quantized_model_path = \"quantized_model.pth\"\n",
    "torch.save(model.state_dict(), fp_32_path)\n",
    "torch.save(qunatized_model.state_dict(), quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ade0258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk ussage of fp32 model: 417.73 MB\n",
      "Disk ussage of quantized model: 173.10 MB\n",
      "Compression ratio: 2.41x\n"
     ]
    }
   ],
   "source": [
    "fp32_model_mb = os.path.getsize(fp_32_path) / (1024 * 1024)\n",
    "quantized_model_mb = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "print(f\"Disk ussage of fp32 model: {fp32_model_mb:.2f} MB\")\n",
    "print(f\"Disk ussage of quantized model: {quantized_model_mb:.2f} MB\")\n",
    "print(f\"Compression ratio: {fp32_model_mb / quantized_model_mb:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e48c72",
   "metadata": {},
   "source": [
    "The FP32 model took 417 MB on disk, while the quantized model took 173 MB, which is about a 2.4× compression ratio. This confirms that weight quantization reduces model size. The lab description states that model size can be reduced by up to 4×, but it depends on the model type, so I think the results in my case are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e38773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_times(model, quantized_model, inputs, device, n_iters=100):\n",
    "    print(\"Checking initial model inference time:\")\n",
    "    original_model_results = compare_pytorch_model_eval_inference_mode(\n",
    "        model, inputs, device, compile_model=False, n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Checking quantized model inference time:\")\n",
    "    quantized_model_results = compare_pytorch_model_eval_inference_mode(\n",
    "        quantized_model, inputs, device, n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    speed_up_ratio = original_model_results[0] / quantized_model_results[0]\n",
    "\n",
    "    print(f\"Speedup with quantized model: {speed_up_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3987aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking initial model inference time:\n",
      "(eval mode + inference_mode) Average inference time over 100 iterations: 0.129314 seconds, std: 0.018578 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Checking quantized model inference time:\n",
      "(eval mode + inference_mode) Average inference time over 100 iterations: 0.059912 seconds, std: 0.004888 seconds\n",
      "\n",
      "Speedup with quantized model: 2.16x\n"
     ]
    }
   ],
   "source": [
    "compare_inference_times(model, qunatized_model, inputs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068077df",
   "metadata": {},
   "source": [
    "Inference time comparison shows that the quantized model is ~2× faster than the initial model, which aligns with the laboratory description claiming a 1.5×–2× speed-up ratio. I evaluated only one mode—using `model.eval()` with `torch.inference_mode()` without checking other combination that were examined previously. However, I expect the results to be similar across different inference modes.\n",
    "\n",
    "Quantization appears to be a useful option for speeding up models in production. However, we need to keep in mind that it is a lossy compression technique. Compressing model weights can lead to reduced accuracy, so we must decide what trade-off is acceptable: a larger model with higher quality but slower inference, or a smaller, faster model with potentially lower quality. If the accuracy drop is small or acceptable for the application, quantization is a great choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7925c9",
   "metadata": {},
   "source": [
    "# 4. GPU optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d44787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a254a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = \"MLOps is great!\"\n",
    "medium_text = \"Transformers are amazing for natural language processing tasks.\"\n",
    "long_text = \"MLOps is one of the best subject in 9t semester. It's not an easy subject, but it provides valuable knowledge about deploying and maintaining machine learning models in production environments. I really enjoy that subcject anf highly recommend it to others interested in the field of machine learning and operations.\"\n",
    "short_inputs = tokenizer(short_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "medium_inputs = tokenizer(\n",
    "    medium_text, return_tensors=\"pt\", truncation=True, padding=True\n",
    ")\n",
    "long_inputs = tokenizer(long_text, return_tensors=\"pt\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compile_mode_inference(\n",
    "    model,\n",
    "    inputs,\n",
    "    device,\n",
    "    compile_mode: Literal[\"default\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n",
    "    n_iters=100,\n",
    "):\n",
    "    model.to(device)\n",
    "    inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
    "    time_lst = []\n",
    "    model.eval()\n",
    "\n",
    "    start = time.time()\n",
    "    compiled_model = torch.compile(model, mode=compile_mode)  # compilation\n",
    "    _ = compiled_model(**inputs_gpu)  # Warm-up after compilation\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Model compilation time and warm-up ({compile_mode}): {end - start:.6f} seconds\"\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(n_iters):\n",
    "            start = time.time()\n",
    "            _ = compiled_model(**inputs_gpu)\n",
    "            end = time.time()\n",
    "            time_lst.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    print(\n",
    "        f\"({compile_mode}) Average inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\"\n",
    "    )\n",
    "    return avg_time, std_time\n",
    "\n",
    "\n",
    "def test_different_compile_modes(model, inputs, device, n_iters=100):\n",
    "    default_results = test_compile_mode_inference(\n",
    "        model, inputs, device, compile_mode=\"default\", n_iters=n_iters\n",
    "    )\n",
    "    autotune_results = test_compile_mode_inference(\n",
    "        model, inputs, device, compile_mode=\"max-autotune\", n_iters=n_iters\n",
    "    )\n",
    "    autotune_no_cudagraphs_results = test_compile_mode_inference(\n",
    "        model,\n",
    "        inputs,\n",
    "        device,\n",
    "        compile_mode=\"max-autotune-no-cudagraphs\",\n",
    "        n_iters=n_iters,\n",
    "    )\n",
    "\n",
    "    autotune_speedup = default_results[0] / autotune_results[0]\n",
    "    autotune_no_cudagraphs_speedup = (\n",
    "        default_results[0] / autotune_no_cudagraphs_results[0]\n",
    "    )\n",
    "    print(f\"Speedup with max-autotune: {autotune_speedup:.2f}x\")\n",
    "    print(\n",
    "        f\"Speedup with max-autotune-no-cudagraphs: {autotune_no_cudagraphs_speedup:.2f}x\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5592892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compilation time and warm-up (default): 41.452659 seconds\n",
      "(default) Average inference time over 1000 iterations: 0.090628 seconds, std: 0.403689 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE bmm(12x7x7, 12x7x64)\n",
      "strides: [49, 7, 1], [64, 768, 1]\n",
      "dtypes: torch.float32, torch.float32\n",
      "  cpp_CppMicroGemmFP32Vec_0 0.0019 ms 100.0% \n",
      "  bmm 0.0223 ms 8.6% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.2543 seconds and 2.4037 seconds precompiling for 2 choices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compilation time and warm-up (max-autotune): 24.445202 seconds\n",
      "(max-autotune) Average inference time over 1000 iterations: 0.092841 seconds, std: 0.461298 seconds\n",
      "Model compilation time and warm-up (max-autotune-no-cudagraphs): 19.472042 seconds\n",
      "(max-autotune-no-cudagraphs) Average inference time over 1000 iterations: 0.093017 seconds, std: 0.470914 seconds\n",
      "Speedup with max-autotune: 0.98x\n",
      "Speedup with max-autotune-no-cudagraphs: 0.97x\n"
     ]
    }
   ],
   "source": [
    "test_different_compile_modes(model, short_inputs, device, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1e79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compilation time and warm-up (default): 49.261932 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1119 17:02:42.729000 14556 torch/_dynamo/convert_frame.py:1016] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1119 17:02:42.729000 14556 torch/_dynamo/convert_frame.py:1016] [0/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/mpnet/modeling_mpnet.py:449)\n",
      "W1119 17:02:42.729000 14556 torch/_dynamo/convert_frame.py:1016] [0/8]    last reason: 0/7: GLOBAL_STATE changed: grad_mode \n",
      "W1119 17:02:42.729000 14556 torch/_dynamo/convert_frame.py:1016] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1119 17:02:42.729000 14556 torch/_dynamo/convert_frame.py:1016] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(default) Average inference time over 1000 iterations: 0.115173 seconds, std: 0.619407 seconds\n",
      "Model compilation time and warm-up (max-autotune): 0.120194 seconds\n",
      "(max-autotune) Average inference time over 1000 iterations: 0.102635 seconds, std: 0.007542 seconds\n",
      "Model compilation time and warm-up (max-autotune-no-cudagraphs): 0.104730 seconds\n",
      "(max-autotune-no-cudagraphs) Average inference time over 1000 iterations: 0.103895 seconds, std: 0.009855 seconds\n",
      "Speedup with max-autotune: 1.12x\n",
      "Speedup with max-autotune-no-cudagraphs: 1.11x\n"
     ]
    }
   ],
   "source": [
    "test_different_compile_modes(model, medium_inputs, device, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e5bbf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compilation time and warm-up (default): 0.215998 seconds\n",
      "(default) Average inference time over 1000 iterations: 0.170848 seconds, std: 0.023272 seconds\n",
      "Model compilation time and warm-up (max-autotune): 0.159622 seconds\n",
      "(max-autotune) Average inference time over 1000 iterations: 0.167672 seconds, std: 0.023845 seconds\n",
      "Model compilation time and warm-up (max-autotune-no-cudagraphs): 0.157727 seconds\n",
      "(max-autotune-no-cudagraphs) Average inference time over 1000 iterations: 0.168254 seconds, std: 0.023500 seconds\n",
      "Speedup with max-autotune: 1.02x\n",
      "Speedup with max-autotune-no-cudagraphs: 1.02x\n"
     ]
    }
   ],
   "source": [
    "test_different_compile_modes(model, long_inputs, device, n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6591c",
   "metadata": {},
   "source": [
    "When comparing the `max-autotune` and `max-autotune-no-cudagraphs` compilation modes with the `default` mode, performance on short input text was slightly worse than the normal mode. This is strange, but it might be because I couldn't find the `pin_memory` parameter in the transformers tokenizer, even though the lab description mentioned it was worth using.\n",
    "\n",
    "For medium input text lengths, there were some gains from using `max-autotune` and `max-autotune-no-cudagraphs`, with speedups of 1.12x and 1.11x respectively. For long input texts, the situation was similar to that of short inputs: results were very close to the `default` mode, though currently slightly better (1.02x speedup).\n",
    "\n",
    "I expected more consistency in the patterns of results and a greater speedup. I also expected that for long sentences, `max-autotune-no-cudagraphs` would perform better because it should allow the model to handle dynamic inputs without recompilation. Maybe something is wrong with my setup, or the test should be performed with batched inputs to better utilize the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dafe4",
   "metadata": {},
   "source": [
    "# 5. Changing numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eda665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 5)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")\n",
    "\n",
    "# tensor cores are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a53301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_time_different_precissions(model, inputs, device, n_iters=100):\n",
    "    print(\"Comparing inference time with different precisions:\")\n",
    "\n",
    "    # FP32\n",
    "    model.to(device)\n",
    "    device_fp32_inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    time_lst_fp32 = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(n_iters):\n",
    "            start = time.time()\n",
    "            _ = model(**device_fp32_inputs)\n",
    "            end = time.time()\n",
    "            time_lst_fp32.append(end - start)\n",
    "    avg_time_fp32 = np.mean(time_lst_fp32)\n",
    "    std_time_fp32 = np.std(time_lst_fp32)\n",
    "    print(\n",
    "        f\"(FP32) Average inference time over {n_iters} iterations: {avg_time_fp32:.6f} seconds, std: {std_time_fp32:.6f} seconds\"\n",
    "    )\n",
    "\n",
    "    # FP16\n",
    "    model_fp16 = model.half().to(device.type)\n",
    "    # NOTE\n",
    "    # For input I don't perform .half() as in input i have indexs_ids and attention_mask. Those value have to be intgerers\n",
    "    # because otherwise embedding layer from transformer will throw error\n",
    "    # it's like we want to retrieve embedding for index 5, but with .half() we converted it to 5.0 float and model doesn't handle that\n",
    "    # same for attention mask, most likely it would be converted to bool_maks, and it also can throw error\n",
    "    device_fp16_inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    time_lst_fp16 = []\n",
    "    model_fp16.eval()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(n_iters):\n",
    "            start = time.time()\n",
    "            _ = model_fp16(**device_fp16_inputs)\n",
    "            end = time.time()\n",
    "            time_lst_fp16.append(end - start)\n",
    "    avg_time_fp16 = np.mean(time_lst_fp16)\n",
    "    std_time_fp16 = np.std(time_lst_fp16)\n",
    "    print(\n",
    "        f\"(FP16) Average inference time over {n_iters} iterations: {avg_time_fp16:.6f} seconds, std: {std_time_fp16:.6f} seconds\"\n",
    "    )\n",
    "\n",
    "    device_autocast_inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    time_lst_autocast = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=True):\n",
    "            for _ in range(n_iters):\n",
    "                start = time.time()\n",
    "                _ = model(**device_autocast_inputs)\n",
    "                end = time.time()\n",
    "                time_lst_autocast.append(end - start)\n",
    "    avg_time_autocast = np.mean(time_lst_autocast)\n",
    "    std_time_autocast = np.std(time_lst_autocast)\n",
    "    print(\n",
    "        f\"(Autocast) Average inference time over {n_iters} iterations: {avg_time_autocast:.6f} seconds, std: {std_time_autocast:.6f} seconds\"\n",
    "    )\n",
    "\n",
    "    speedup_fp_16 = avg_time_fp32 / avg_time_fp16\n",
    "    speedup_autocast = avg_time_fp32 / avg_time_autocast\n",
    "    print(f\"Speedup with FP16: {speedup_fp_16:.2f}x\")\n",
    "    print(f\"Speedup with Autocast: {speedup_autocast:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc19d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing inference time with different precisions:\n",
      "(FP32) Average inference time over 100 iterations: 0.012918 seconds, std: 0.049498 seconds\n",
      "(FP16) Average inference time over 100 iterations: 0.009761 seconds, std: 0.018628 seconds\n",
      "(Autocast) Average inference time over 100 iterations: 0.009600 seconds, std: 0.001223 seconds\n",
      "Speedup with FP16: 1.32x\n",
      "Speedup with Autocast: 1.35x\n"
     ]
    }
   ],
   "source": [
    "compare_inference_time_different_precissions(model, inputs, device, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ffd8d",
   "metadata": {},
   "source": [
    "Both half precision and torch Autocast speed up inference by approximately 1.35x. In practice, I prefer `Autocast` mode because it seems slightly faster and safer, as it lets PyTorch handle precision casting automatically.\n",
    "\n",
    "I also encountered a tricky scenario when using the `.half()` method. I couldn't apply it to my inputs because they were token IDs and attention masks. Since these values must remain integers, mapping them to fp16 threw an error in my model. Therefore, we must carefully consider input types and model requirements when changing numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba42c5",
   "metadata": {},
   "source": [
    "# 6. ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f96ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d0ca80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-435139328.py:19: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "# loading model and tokenizer to make sure there are prepared and no overwritten\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")\n",
    "\n",
    "# exporting model to onnx\n",
    "device = \"cpu\"\n",
    "# Put the model in eval mode and move to CPU\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "# Example input for tracking (for onnx export)\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7428ce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.onnx  sample_data\n"
     ]
    }
   ],
   "source": [
    "# Make sure that onnx model is saved\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fba473ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.10015495, -0.24121654, -0.07321592, ...,  0.07796387,\n",
      "         -0.21866202, -0.2590373 ],\n",
      "        [ 0.14760065, -0.13897762, -0.14370751, ..., -0.01106242,\n",
      "         -0.2662107 , -0.12384814],\n",
      "        [ 0.08314157, -0.223754  , -0.14671417, ..., -0.09309337,\n",
      "         -0.1952658 , -0.19207902],\n",
      "        ...,\n",
      "        [-0.0013935 ,  0.2468841 ,  0.02294915, ...,  0.04206365,\n",
      "         -0.30782154, -0.23525402],\n",
      "        [ 0.09198561, -0.18079105,  0.00285578, ...,  0.04924215,\n",
      "         -0.17577925, -0.2860438 ],\n",
      "        [ 0.0876145 , -0.20475739, -0.05056114, ...,  0.11682078,\n",
      "         -0.2020796 , -0.2533966 ]]], dtype=float32), array([[ 1.52873486e-01, -5.95023558e-02, -6.43299222e-02,\n",
      "         8.02019760e-02, -4.88571338e-02, -2.81683151e-02,\n",
      "         5.04173413e-02,  8.66596587e-03, -5.80217317e-02,\n",
      "        -1.00995872e-04,  3.33862961e-03,  3.56380381e-02,\n",
      "         1.18111536e-01, -7.24004731e-02,  4.40806411e-02,\n",
      "         1.08006254e-01, -1.30637512e-01,  5.13480194e-02,\n",
      "         1.19970694e-01,  4.56414744e-02,  1.11562155e-01,\n",
      "        -7.39118978e-02,  2.63867769e-02, -1.30375102e-02,\n",
      "         3.60144977e-03,  1.58626124e-01, -3.93018499e-02,\n",
      "         4.00619991e-02, -1.31829143e-01, -9.41019058e-02,\n",
      "        -1.35642439e-01, -1.46097273e-01, -1.80394515e-01,\n",
      "         9.35631432e-03, -5.35258986e-02, -2.72901859e-02,\n",
      "         5.81860840e-02,  1.92325041e-02, -5.31388670e-02,\n",
      "         6.89660609e-02,  7.40158781e-02,  1.66807279e-01,\n",
      "         1.59200057e-02,  1.20913826e-01,  4.37551960e-02,\n",
      "        -4.18233797e-02, -9.59790498e-02, -9.85372905e-03,\n",
      "        -1.16053209e-01,  8.10248181e-02,  3.77274342e-02,\n",
      "         9.02676955e-02, -3.90624367e-02, -9.53958184e-02,\n",
      "         1.60198674e-01, -1.83515713e-01,  6.47233129e-02,\n",
      "         1.24616623e-01, -1.55205563e-01, -9.97582301e-02,\n",
      "         2.41008885e-02, -9.78943929e-02,  1.44119933e-01,\n",
      "        -1.75846983e-02,  9.78874713e-02,  1.00149913e-02,\n",
      "         6.10274412e-02, -5.63673340e-02,  6.65725470e-02,\n",
      "        -1.00782521e-01, -4.91213845e-03, -3.81100252e-02,\n",
      "        -5.53194545e-02,  1.05895307e-02, -4.33139727e-02,\n",
      "        -1.08455606e-01,  3.82905640e-02, -1.59183424e-02,\n",
      "        -5.24236634e-02,  1.02931447e-02,  1.30359326e-02,\n",
      "         3.70292626e-02,  4.17776071e-02, -6.54343367e-02,\n",
      "        -1.71335302e-02, -4.89844084e-02, -7.18914270e-02,\n",
      "        -2.07423959e-02,  4.10505980e-02, -1.56374350e-01,\n",
      "        -8.98887664e-02, -1.74670517e-02,  7.47979758e-03,\n",
      "        -2.08024755e-02,  2.80061811e-02, -1.18120432e-01,\n",
      "         2.15845536e-02,  7.98194334e-02, -3.67024839e-02,\n",
      "        -3.58537585e-02,  1.17941070e-02,  1.81509536e-02,\n",
      "         4.29143421e-02, -3.30975465e-02,  8.84776562e-02,\n",
      "         5.80634326e-02,  1.06361955e-01, -1.12738088e-02,\n",
      "         8.24952796e-02, -2.19488777e-02, -8.70009288e-02,\n",
      "         3.89030576e-02,  7.23423436e-02,  5.20453416e-02,\n",
      "         2.95564849e-02,  1.30852640e-01, -3.23310345e-02,\n",
      "        -4.59714867e-02, -2.28088871e-02,  1.74671579e-02,\n",
      "         4.83319815e-03, -6.76368698e-02,  1.57109156e-01,\n",
      "        -9.23929587e-02,  9.19620022e-02,  2.62097758e-03,\n",
      "        -1.69068813e-01,  3.56987305e-02, -3.30905356e-02,\n",
      "         1.13757864e-01,  7.31737167e-02, -6.94043338e-02,\n",
      "        -2.98331887e-03, -8.78576469e-03, -3.95255424e-02,\n",
      "        -1.48472823e-02, -6.40719235e-02,  8.34998041e-02,\n",
      "         1.10578015e-02, -2.65729856e-02,  1.33219445e-02,\n",
      "        -3.16707743e-03, -8.12610686e-02,  1.16950674e-02,\n",
      "        -1.82852596e-01, -4.85816784e-02,  9.25395563e-02,\n",
      "         2.34599300e-02,  2.13895082e-01, -6.54521510e-02,\n",
      "        -6.00673705e-02,  2.14703474e-02, -3.09819113e-02,\n",
      "        -2.85708196e-02,  5.97157981e-03, -1.77643538e-01,\n",
      "        -3.58514003e-02,  9.51116681e-02, -1.41209457e-02,\n",
      "        -1.26557261e-01, -8.34019333e-02,  6.79912418e-02,\n",
      "         1.57451361e-01, -3.65004502e-03, -1.04726478e-01,\n",
      "         7.85345957e-02,  7.17330873e-02,  2.53974557e-01,\n",
      "         4.81103687e-03, -6.01369776e-02, -8.45557079e-02,\n",
      "         3.20063136e-03,  3.74405272e-02,  2.80990284e-02,\n",
      "         5.59707312e-03,  8.77961442e-02, -7.39006558e-03,\n",
      "        -1.60621822e-01, -1.14528321e-01, -9.23316032e-02,\n",
      "        -8.91041383e-03,  1.64544303e-02,  5.91184162e-02,\n",
      "        -5.85079752e-02,  6.70795441e-02, -5.87571934e-02,\n",
      "         1.21293470e-01,  1.73333089e-03,  1.23595744e-01,\n",
      "        -3.52012888e-02, -4.16489830e-03, -4.97093983e-02,\n",
      "         4.92992960e-02, -1.15600258e-01,  4.50243289e-03,\n",
      "         3.54831256e-02,  6.71747024e-04, -1.19749397e-01,\n",
      "        -4.82223555e-02,  3.39585096e-02, -1.64028525e-01,\n",
      "        -4.72728051e-02, -1.13227442e-01, -9.76282060e-02,\n",
      "        -1.93987824e-02,  1.18941538e-01, -1.83574036e-02,\n",
      "         1.36290595e-01, -6.02766797e-02, -3.83296870e-02,\n",
      "        -4.71067950e-02,  7.06707919e-03, -9.04786289e-02,\n",
      "         3.28894630e-02,  5.51765636e-02,  4.71572168e-02,\n",
      "         9.46111009e-02,  9.72933099e-02,  2.47863475e-02,\n",
      "         6.77273348e-02, -5.71014583e-02, -5.41886948e-02,\n",
      "        -1.80812597e-01, -2.90515367e-03,  1.01654483e-02,\n",
      "         7.12337121e-02,  5.61298579e-02,  1.55853545e-02,\n",
      "         1.01709895e-01,  1.09398194e-01, -1.72590166e-02,\n",
      "         2.27035340e-02, -6.32868856e-02,  5.79274818e-02,\n",
      "        -2.33821268e-03,  1.69509370e-02, -8.36683586e-02,\n",
      "         6.22208305e-02,  2.45539211e-02,  3.47689427e-02,\n",
      "         6.03122404e-03,  8.33689347e-02, -3.01289000e-02,\n",
      "         2.88309623e-02,  1.79862790e-02,  1.10734515e-01,\n",
      "        -5.63296787e-02,  1.05960639e-02,  4.62482870e-02,\n",
      "         2.64146831e-02,  6.73484355e-02, -2.67093536e-02,\n",
      "        -5.40149435e-02,  1.09422587e-01, -9.68122259e-02,\n",
      "        -1.02735206e-01,  5.32942750e-02, -1.21936481e-02,\n",
      "        -3.46709117e-02,  9.22636464e-02, -1.73879031e-03,\n",
      "        -3.31454426e-02, -6.18775934e-02, -1.21101663e-02,\n",
      "         1.23629961e-02,  1.03922389e-01,  8.46740529e-02,\n",
      "        -2.92590596e-02,  4.24983911e-02,  3.47522758e-02,\n",
      "         1.07094243e-01, -1.24608977e-02,  1.58572253e-02,\n",
      "         1.21843098e-02,  5.57916723e-02, -2.02182005e-03,\n",
      "         8.05254735e-04,  7.21463636e-02,  6.70088530e-02,\n",
      "         6.59086183e-02,  1.12952232e-01, -5.08812703e-02,\n",
      "        -8.97771791e-02, -6.38815761e-02, -6.06362596e-02,\n",
      "         9.30848811e-03, -1.82513241e-02, -3.76271717e-02,\n",
      "         2.47660577e-02, -2.19968762e-02,  3.11863106e-02,\n",
      "        -8.92098621e-02,  6.45223781e-02,  1.69593114e-02,\n",
      "         1.90359000e-02,  1.54973984e-01,  1.41777834e-02,\n",
      "         2.35221032e-02,  3.85722928e-02, -3.42606157e-02,\n",
      "         4.78928387e-02, -6.13196343e-02, -2.88159400e-02,\n",
      "        -3.17572989e-02,  2.77048983e-02, -1.20052889e-01,\n",
      "         6.03954867e-02, -1.34931235e-02,  1.79285707e-03,\n",
      "        -2.10456420e-02, -1.58865377e-02, -1.17464475e-01,\n",
      "        -1.95009373e-02,  3.38072330e-02,  2.43503265e-02,\n",
      "         2.45138500e-02,  4.01750281e-02,  1.14416786e-01,\n",
      "        -1.24614038e-01, -1.93938278e-02,  7.37611484e-03,\n",
      "         4.12917398e-02,  1.54131167e-02, -5.39302938e-02,\n",
      "         1.98404212e-03,  7.80544523e-03,  3.01089026e-02,\n",
      "         1.18279122e-02,  2.01041270e-02,  8.38764682e-02,\n",
      "         2.30034105e-02, -1.45618450e-02, -1.05149463e-01,\n",
      "         6.39351457e-02,  2.26852000e-01,  8.42036977e-02,\n",
      "         2.31858715e-02,  1.08660683e-01,  4.57847714e-02,\n",
      "         3.82653512e-02,  3.69253829e-02, -2.80232504e-02,\n",
      "        -5.55796698e-02, -2.38710400e-02,  1.10185832e-01,\n",
      "         6.29637241e-02,  5.45681128e-03,  3.98241132e-02,\n",
      "        -8.56714621e-02,  1.25804737e-01,  3.62589210e-02,\n",
      "         3.76017578e-02,  4.46886718e-02,  1.05780475e-02,\n",
      "         5.54876286e-04,  1.07769117e-01, -1.96618624e-02,\n",
      "         5.48500568e-02,  4.78228219e-02, -3.19819413e-02,\n",
      "        -5.50605245e-02, -5.70502318e-02, -3.27275395e-02,\n",
      "         8.81667659e-02, -5.86916842e-02, -4.73439395e-02,\n",
      "        -1.42570063e-02,  2.01058928e-02,  1.33293020e-02,\n",
      "         6.29354417e-02, -1.49169624e-01,  5.08708619e-02,\n",
      "         3.77898440e-02, -2.86013596e-02,  2.71086972e-02,\n",
      "         8.72227773e-02,  5.54564744e-02, -4.94586527e-02,\n",
      "         1.34211089e-02,  4.13086899e-02,  3.88434753e-02,\n",
      "         1.53276017e-02,  8.27623904e-02, -9.66900587e-02,\n",
      "        -8.54638368e-02, -8.23838785e-02, -8.85893255e-02,\n",
      "         2.39279750e-03,  1.64826006e-01, -4.80098948e-02,\n",
      "        -1.88336335e-02, -1.33018896e-01, -5.27979694e-02,\n",
      "        -8.55074525e-02,  3.30907363e-03,  5.35017774e-02,\n",
      "         5.70689365e-02,  2.67964173e-02,  3.65750827e-02,\n",
      "        -6.03627088e-03,  9.56947803e-02,  7.04139173e-02,\n",
      "        -3.78197134e-02, -5.44276647e-02,  1.16856014e-02,\n",
      "         1.35377035e-04, -1.18664438e-02,  7.91780874e-02,\n",
      "        -1.96496118e-02, -4.17163670e-02,  2.38965638e-02,\n",
      "        -7.74755254e-02,  1.03541175e-02,  1.33070005e-02,\n",
      "        -2.75460780e-02,  1.33358393e-04, -3.64122801e-02,\n",
      "        -1.13141410e-01,  1.00741193e-01, -9.45856720e-02,\n",
      "         1.04727730e-01, -1.20449206e-02,  1.74722731e-01,\n",
      "         6.44276291e-02,  9.42763165e-02,  9.53622609e-02,\n",
      "        -3.78526598e-02, -6.84494451e-02,  5.84452376e-02,\n",
      "        -8.37687925e-02,  4.39093187e-02, -9.69319344e-02,\n",
      "        -5.18962033e-02,  1.86023172e-02,  5.74264079e-02,\n",
      "         1.92717716e-01, -4.49947227e-04, -3.81504223e-02,\n",
      "        -7.50578418e-02,  2.26198509e-02, -4.87929620e-02,\n",
      "        -6.50182888e-02, -7.18758330e-02, -1.80901155e-01,\n",
      "         8.47553462e-02,  7.27661029e-02, -5.71573973e-02,\n",
      "        -1.14957930e-03,  1.04428589e-01,  3.95210274e-02,\n",
      "         6.34412989e-02,  2.92894971e-02,  1.70352519e-01,\n",
      "         6.23086728e-02,  7.94044137e-02, -1.12285219e-01,\n",
      "        -9.99952033e-02,  3.78289036e-02,  2.32145846e-01,\n",
      "         9.25827548e-02, -4.57340553e-02, -7.93865621e-02,\n",
      "         3.16696540e-02,  4.01351973e-02, -3.75255868e-02,\n",
      "         1.61531214e-02,  3.62246744e-02,  1.60304070e-01,\n",
      "        -5.11321016e-02,  5.33313602e-02,  9.94264632e-02,\n",
      "        -1.06096521e-01,  4.65275310e-02, -4.32701483e-02,\n",
      "        -1.08155631e-01, -1.06288671e-01,  2.42149103e-02,\n",
      "         6.30335733e-02, -3.16143222e-02, -1.55583993e-01,\n",
      "        -1.18604422e-01, -1.07127100e-01, -2.22013071e-02,\n",
      "        -1.04168229e-01,  2.30305572e-03,  2.72944309e-02,\n",
      "        -1.15312357e-02,  6.64366707e-02,  6.87418655e-02,\n",
      "        -2.57528946e-02,  5.73403686e-02, -9.08820406e-02,\n",
      "        -1.91882223e-01, -1.00676790e-01,  2.58530099e-02,\n",
      "         1.15209753e-02,  9.03390273e-02, -1.44938067e-01,\n",
      "        -3.20745632e-02,  4.98409988e-03,  8.58685151e-02,\n",
      "         2.31740493e-02,  5.96712716e-02, -1.40692834e-02,\n",
      "        -6.58665672e-02,  7.17430115e-02,  1.37329727e-01,\n",
      "         1.97924562e-02,  2.54863761e-02,  6.06903806e-02,\n",
      "        -3.36715169e-02,  1.04161397e-01, -4.15627360e-02,\n",
      "         1.17442071e-01,  6.54770583e-02, -1.28186233e-02,\n",
      "        -1.02034090e-02,  8.47127438e-02,  6.03936352e-02,\n",
      "        -1.08521618e-01, -4.76167761e-02, -2.06653625e-02,\n",
      "        -4.96007428e-02,  8.90772343e-02,  3.17545980e-02,\n",
      "         1.82698723e-02, -1.00827731e-01, -4.53281701e-02,\n",
      "        -2.02017277e-01,  6.11810274e-02, -1.11126304e-01,\n",
      "         1.12161599e-01,  4.52282578e-02, -1.87826026e-02,\n",
      "         9.39599499e-02, -2.33872347e-02, -4.91797365e-02,\n",
      "        -4.47003059e-02, -1.78876698e-01, -3.00525557e-02,\n",
      "        -5.85840596e-03, -1.34614691e-01, -8.23979452e-02,\n",
      "        -1.49731293e-01, -7.06270933e-02,  4.06806171e-02,\n",
      "         5.79014532e-02, -1.05775483e-01,  1.03856483e-03,\n",
      "        -1.10918052e-01, -8.18918832e-03,  8.90885107e-03,\n",
      "        -7.26932660e-02,  8.86139870e-02, -1.14098117e-02,\n",
      "         5.68416715e-02, -7.55799562e-02,  8.87088105e-03,\n",
      "         9.52603891e-02,  3.21961902e-02, -2.58126855e-02,\n",
      "         5.79504333e-02,  6.79877922e-02, -6.10065460e-02,\n",
      "        -2.06439998e-02,  4.25303355e-02,  1.29702017e-01,\n",
      "        -1.60408411e-02,  6.77992031e-02,  1.13115124e-01,\n",
      "         2.66578309e-02, -6.38979115e-03,  4.05991413e-02,\n",
      "        -7.90872425e-02,  5.13041206e-02,  1.30205721e-01,\n",
      "         1.50170207e-01, -5.37733920e-02, -1.64222121e-01,\n",
      "         7.03317870e-04, -3.19709890e-02,  2.91335881e-02,\n",
      "         3.01907305e-02,  4.34302799e-02,  2.02213656e-02,\n",
      "         8.31672642e-03, -3.21285054e-02, -1.39437085e-02,\n",
      "        -5.67433250e-04, -8.75475630e-03, -3.86869311e-02,\n",
      "        -7.74261802e-02, -6.42162189e-02,  4.09442873e-04,\n",
      "         1.96251869e-02,  4.48376499e-02, -9.24657583e-02,\n",
      "        -3.86579260e-02, -5.72033934e-02,  1.38228917e-02,\n",
      "         5.94281927e-02,  7.41309747e-02, -3.44106220e-02,\n",
      "        -1.11071179e-02, -1.72201935e-02, -1.00815937e-01,\n",
      "         1.05111087e-02,  1.57747552e-01, -5.99889308e-02,\n",
      "         2.82375654e-03, -8.15264229e-03, -7.82679319e-02,\n",
      "        -4.95488308e-02,  1.19555853e-01, -3.25757712e-02,\n",
      "         2.29720362e-02,  3.78100500e-02, -8.38607997e-02,\n",
      "        -6.04859851e-02, -2.95005702e-05,  9.65541750e-02,\n",
      "         4.62689251e-03,  5.87397106e-02,  1.43968714e-02,\n",
      "         6.59944909e-03,  1.29985541e-01,  1.39867201e-01,\n",
      "         2.69494541e-02, -4.13668640e-02, -5.88789918e-02,\n",
      "         1.26911607e-02, -8.46075919e-03,  5.82536869e-02,\n",
      "        -1.06741069e-02,  1.51655421e-01, -1.62826274e-02,\n",
      "         3.44416592e-03, -1.53745187e-03,  1.14255957e-02,\n",
      "        -1.10200532e-01,  3.46796529e-04, -2.25576796e-02,\n",
      "        -9.18179099e-03,  3.80430594e-02, -1.56189919e-01,\n",
      "        -4.49109450e-02, -4.08027582e-02,  5.49516715e-02,\n",
      "         3.55254076e-02,  2.41808564e-04, -1.72650367e-01,\n",
      "         3.93960737e-02, -1.21621657e-02, -3.37066203e-02,\n",
      "         3.81465070e-02,  9.72247124e-02, -3.02397478e-02,\n",
      "         2.46111155e-02, -1.91000092e-03,  5.48384385e-03,\n",
      "         1.27718270e-01,  1.20747834e-01, -7.65872151e-02,\n",
      "        -4.37317304e-02, -3.53176706e-02,  8.17463100e-02,\n",
      "         8.74801725e-02,  3.79824452e-02, -7.49097159e-03,\n",
      "        -1.37912761e-03, -5.00747412e-02,  4.61991020e-02,\n",
      "        -1.05478952e-03,  5.23947589e-02,  8.15160424e-02,\n",
      "         1.62222777e-02,  3.40268984e-02, -4.17257100e-02,\n",
      "        -1.59069709e-02,  1.04347534e-01, -1.75966993e-02,\n",
      "        -4.39037848e-03, -6.85333237e-02,  2.71103950e-03,\n",
      "        -5.17306589e-02, -8.03883076e-02,  7.03248009e-02,\n",
      "         2.70363735e-03,  8.69402383e-03, -8.42628721e-03,\n",
      "         9.46888551e-02, -4.19272576e-03, -3.36521491e-02,\n",
      "         3.00489087e-02, -1.11623236e-03,  2.36245655e-02,\n",
      "        -4.86333482e-02, -5.18613718e-02, -1.09962365e-02,\n",
      "         2.65096202e-02, -1.16942726e-01,  3.24263945e-02,\n",
      "         4.56585847e-02,  8.50227028e-02,  1.35129154e-01,\n",
      "        -3.11611444e-02,  8.33934620e-02, -5.06081656e-02,\n",
      "        -3.36481445e-02, -1.82699412e-02, -1.62406601e-02,\n",
      "        -5.02214693e-02, -1.25247159e-03, -8.47014040e-03,\n",
      "         1.72110628e-02, -8.46542194e-02,  9.55546182e-03,\n",
      "         1.07913548e-02, -6.30123913e-02, -3.18597630e-02,\n",
      "        -6.83431700e-02,  5.28517738e-02,  7.89692029e-02,\n",
      "        -3.50214131e-02, -1.22034494e-02,  3.55020203e-02,\n",
      "        -2.34777946e-02,  2.69679166e-02, -3.11722271e-02,\n",
      "        -6.53575659e-02, -2.32648067e-02, -1.11328639e-01,\n",
      "         5.12311086e-02,  1.05759539e-01,  3.71308699e-02,\n",
      "         3.20880711e-02,  5.84048666e-02, -1.02994926e-01,\n",
      "         2.15133522e-02,  3.01788151e-02, -3.86731662e-02,\n",
      "         1.34872198e-02,  1.19734639e-02, -4.28073704e-02,\n",
      "         9.92826670e-02, -5.86380735e-02,  5.81089233e-04,\n",
      "        -1.02907911e-01,  1.49102986e-01, -1.11552021e-02,\n",
      "        -3.33144865e-03,  3.02118249e-02,  1.79495499e-01,\n",
      "        -5.69037162e-02, -1.57684013e-02,  2.73438962e-03,\n",
      "        -1.37083128e-01,  2.62835976e-02,  1.33331254e-01,\n",
      "         2.16071364e-02, -3.30083887e-03, -4.16270792e-02,\n",
      "         1.93104260e-02, -1.62004083e-02, -5.55836083e-03,\n",
      "        -9.95214842e-03,  1.77427065e-02,  6.13812655e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Prepare input data\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create input dictionary, in same format as during export\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "# Run inference\n",
    "outputs_onnx = ort_session.run(None, inputs_onnx)\n",
    "print(outputs_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1799ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7b763eb5ca10>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving optimized onxx model for offline optimization\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Explicitly set optimization level to ALL (it's setup by default but just to play with it)\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Save the optimized model to this path\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "\n",
    "# Create InferenceSession, which will perform offline optimization and save the optimized model\n",
    "# explicitly setup cpu provider as we are testing cpu in that example\n",
    "ort.InferenceSession(\"model.onnx\", sess_options, providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cbd6516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.onnx  model_optimized.onnx  sample_data\n"
     ]
    }
   ],
   "source": [
    "# Make sure that optimized model is saved\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35898820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_onnx_coldsart(\n",
    "    onnx_model_path: str, inputs, optimization_mode: Literal[\"online\", \"offline\"]\n",
    "):\n",
    "    start = time.time()\n",
    "    sess_options = ort.SessionOptions()\n",
    "\n",
    "    if optimization_mode == \"online\":\n",
    "        # enable for online mode\n",
    "        sess_options.graph_optimization_level = (\n",
    "            ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        )\n",
    "    else:\n",
    "        # disable for offline mode\n",
    "        sess_options.graph_optimization_level = (\n",
    "            ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "        )\n",
    "\n",
    "    # explicitly setup cpu provider as we are testing cpu in that example\n",
    "    _ = ort.InferenceSession(\n",
    "        onnx_model_path, sess_options=sess_options, providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"ONNX Runtime coldstart ({optimization_mode} optimization): {end - start:.6f} seconds\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "514b2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime coldstart (online optimization): 0.978385 seconds\n",
      "ONNX Runtime coldstart (offline optimization): 0.589654 seconds\n"
     ]
    }
   ],
   "source": [
    "measure_onnx_coldsart(\"model.onnx\", sample_input, optimization_mode=\"online\")\n",
    "measure_onnx_coldsart(\"model_optimized.onnx\", sample_input, optimization_mode=\"offline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcd5bf",
   "metadata": {},
   "source": [
    "As we can observe, cold start for online mode is larger as in that case each time we create `ort.InferenceSession` object we apply optimization on onnx model. There is also some trick I found in description that, for exporting model to `onnx` we use `pt` type tensors but later with inference we are using `np` type tensor. I believe that happens because `onnx` format is framework/language agnostic and we don't want to use pytorch `pt` tensor types (or even we can't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "89322906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_onnx_inference_time(\n",
    "    onnx_model_path,\n",
    "    inputs,\n",
    "    optimization_mode: Literal[\"online\", \"offline\"],\n",
    "    n_iters=100,\n",
    "):\n",
    "    sess_options = ort.SessionOptions()\n",
    "\n",
    "    if optimization_mode == \"online\":\n",
    "        # enable for online mode\n",
    "        sess_options.graph_optimization_level = (\n",
    "            ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        )\n",
    "    else:\n",
    "        # disable for offline mode\n",
    "        sess_options.graph_optimization_level = (\n",
    "            ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "        )\n",
    "\n",
    "    # Create InferenceSession\n",
    "    # explicitly setup cpu provider as we are testing cpu in that example\n",
    "    session_cpu = ort.InferenceSession(\n",
    "        onnx_model_path, sess_options=sess_options, providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "\n",
    "    # Prepare inputs for ONNX Runtime\n",
    "    inputs_onnx = {\n",
    "        \"input_ids\": sample_input[\"input_ids\"],\n",
    "        \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "    # Measure inference time\n",
    "    time_lst = []\n",
    "    for _ in range(n_iters):\n",
    "        start = time.time()\n",
    "        _ = session_cpu.run(None, inputs_onnx)\n",
    "        end = time.time()\n",
    "        time_lst.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(time_lst)\n",
    "    std_time = np.std(time_lst)\n",
    "    print(\n",
    "        f\"({optimization_mode}) Average ONNX inference time over {n_iters} iterations: {avg_time:.6f} seconds, std: {std_time:.6f} seconds\"\n",
    "    )\n",
    "    return avg_time, std_time\n",
    "\n",
    "\n",
    "def compare_onnx_modes_inference_times(inputs, n_iters=100):\n",
    "    online_results = measure_onnx_inference_time(\n",
    "        \"model.onnx\", inputs, optimization_mode=\"online\", n_iters=n_iters\n",
    "    )\n",
    "    offline_results = measure_onnx_inference_time(\n",
    "        \"model_optimized.onnx\", inputs, optimization_mode=\"offline\", n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    speedup_offline = online_results[0] / offline_results[0]\n",
    "    print(f\"Speedup with offline optimized ONNX model: {speedup_offline:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bd19c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(online) Average ONNX inference time over 1000 iterations: 0.041703 seconds, std: 0.003944 seconds\n",
      "(offline) Average ONNX inference time over 1000 iterations: 0.041459 seconds, std: 0.003448 seconds\n",
      "Speedup with offline optimized ONNX model: 1.01x\n"
     ]
    }
   ],
   "source": [
    "compare_onnx_modes_inference_times(sample_input, n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df2ac7",
   "metadata": {},
   "source": [
    "There are no differences in inference time between `online` mode and `offline` mode. This happens because the difference only occurs during the cold start when the `ort.InferenceSession` object is created. After that, the inference times are the same because both models are equally optimized after the cold start, regardless of whether `online` or `offline` mode was used. In my benchmark, I measure only the inference time without the cold start, as the cold start was measured previously. This explains why no difference appears in this benchmark - the performance difference only shows up during the cold start phase, which was measured separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49cab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")\n",
    "inputs = tokenizer(\n",
    "    [\"test text for checking outputs\"],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00977f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/dawidwozniak/studies/sem9/MLOps/lab07/laboratory/.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:1114: RuntimeWarning: Guards may run slower on Python 3.13.0. Consider upgrading to Python 3.13.1+.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# saving pytorch compiled model. Onnx ofline optimized model will be used from above execution\n",
    "model.eval()\n",
    "compiled_model = torch.compile(model)\n",
    "with torch.inference_mode():\n",
    "    # warmup\n",
    "    _ = compiled_model(**inputs)\n",
    "torch.save(compiled_model.state_dict(), \"compiled_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c14faa",
   "metadata": {},
   "source": [
    "![image.png](assets/image_sizes.png)\n",
    "\n",
    "The Docker image for ONNX is lighter, as we don’t need PyTorch for pure ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3391e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time over 100 runs: 0.033357372283935545 seconds\n",
      "Stddev of inference time over 100 runs: 0.00849718876790392 seconds\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm pytorch-model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f9fbd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime cpuid_info warning: Unknown CPU vendor. cpuinfo_vendor value: 0\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Average inference time over 100 runs: 0.015625667572021485 seconds\n",
      "Stddev of inference time over 100 runs: 0.0034382682264247942 seconds\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm onnx-model     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383e7c0",
   "metadata": {},
   "source": [
    "For the Docker test, I used:\n",
    "\n",
    "- an ONNX model with offline optimization\n",
    "\n",
    "- a PyTorch compiled model (I didn’t use the quantized version to keep the same model size and weight precision)\n",
    "\n",
    "The ONNX configuration was faster: average inference time was 0.015 seconds vs. 0.03 seconds for pytorch. I think it is preferable for production to use ONNX because it’s a framework-agnostic approach, allows us to save resources by not explicitly adding PyTorch to the Docker image, and it’s faster.\n",
    "\n",
    "The ONNX configuration can be found in `Dockerfile.onnx` and `serve_onnx_model.py`.\n",
    "The PyTorch configuration can be found in `Dockerfile.pytorch` and `serve_pytorch_model.py`.\n",
    "\n",
    "Please note that the absolute execution times in this case may differ from the previous tests, because the earlier tests were performed in a Google Colab environment, while the Docker-based tests were carried out in a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b20bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
